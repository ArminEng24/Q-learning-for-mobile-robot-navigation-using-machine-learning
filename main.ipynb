{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6b48526",
   "metadata": {},
   "source": [
    "# Task 3: Q-learning for mobile robot navigation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1c18f9",
   "metadata": {},
   "source": [
    "## Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b3499a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing numpy library\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# auxiliary functions\n",
    "\n",
    "def action_decode(act_code):\n",
    "    dirs = {0: \"N\", 1: \"E\", 2: \"S\", 3: \"W\"}\n",
    "    return dirs[act_code]\n",
    "\n",
    "def action_encode(act):\n",
    "    dir_codes = {\"N\": 0, \"E\": 1, \"S\": 2, \"W\": 3}\n",
    "    return dir_codes(act)\n",
    "\n",
    "def display_learning(series, label):\n",
    "    n_episodes = len(series)\n",
    "    show_n = 20\n",
    "    show_step = int(n_episodes/show_n)\n",
    "    sequence = []\n",
    "    for i in range(show_n):\n",
    "        sequence.append(np.mean(series[show_step*i:show_step*(i+1)]))\n",
    "        print((i+1) * show_step, ' episodes ', label, sequence[-1])\n",
    "    print('\\n')\n",
    "    plt.figure()\n",
    "    plt.plot(sequence)\n",
    "    plt.ylabel(label)\n",
    "    plt.xlabel('episodes')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8db484b",
   "metadata": {},
   "source": [
    "## Grid Environment Class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e581bb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class grid_env():\n",
    "# definition of the maze environment\n",
    "\n",
    "    def __init__(self, width = 5, height = 5, start = [0, 0], debug = False):\n",
    "        # Contructor methods create the environment with some given options\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.start = start\n",
    "        self.goal = [self.width - 1, self.height - 1]\n",
    "        self.debug = debug\n",
    "        self.n_states = self.width * self.height\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        # Reset method puts the state at the starting position\n",
    "        self.pos = self.start[:]   # columns, rows\n",
    "        return self.pos, 0, False        \n",
    "\n",
    "    def state_decode(self, obs_code):\n",
    "        r = obs_code // self.width\n",
    "        c = obs_code % self.width\n",
    "        return([c, r])\n",
    "    \n",
    "    def state_encode(self, position):\n",
    "        code = position[0] + position[1] * (self.width) # columns, rows\n",
    "        return(code)\n",
    "\n",
    "    def step(self, action):\n",
    "        # Depending on the action, update the environment state\n",
    "        if action == \"S\" and (self.pos[1] < self.height -1):\n",
    "            self.pos[1] += 1\n",
    "        elif action == \"N\" and self.pos[1] > 0:\n",
    "            self.pos[1] -= 1\n",
    "        elif action == \"W\" and self.pos[0] > 0:\n",
    "            self.pos[0] -= 1\n",
    "        elif action == \"E\" and (self.pos[0] < self.width -1):\n",
    "            self.pos[0] += 1\n",
    "\n",
    "        done = (self.pos == self.goal)  # check if goal was reached\n",
    "        if done:\n",
    "            reward = self.width + self.height  # reward at goal\n",
    "        else:\n",
    "            reward = -1  # negative reward at every step\n",
    "\n",
    "        if self.debug:\n",
    "            print(self.render())\n",
    "\n",
    "        return self.pos, reward, done\n",
    "\n",
    "    def render(self):\n",
    "        res = \"\"\n",
    "        for y in range(self.height):\n",
    "            for x in range(self.width):\n",
    "                if self.goal[0] == x and self.goal[1] == y:\n",
    "                    if self.pos[0] == x and self.pos[1] == y:\n",
    "                        res += \"@\"\n",
    "                    else:\n",
    "                        res += \"o\"\n",
    "                    continue\n",
    "                if self.pos[0] == x and self.pos[1] == y:\n",
    "                    res += \"x\"\n",
    "                else:\n",
    "                    res += \"_\"\n",
    "            res += \"\\n\"\n",
    "        return(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5dbc9a",
   "metadata": {},
   "source": [
    "## Agent Class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f386e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class agent():\n",
    "# definition of the agent\n",
    "\n",
    "    def __init__(self, n_obs, discount = 1, learning_rate = 0.1, eps = {'start': 1, 'min': 0.01, 'decay': 0.001}):\n",
    "        self.action_space = np.asarray([0, 1, 2, 3])  # north, east, south, west\n",
    "        n_actions = np.shape(self.action_space)[0]\n",
    "        self.Q_table = np.zeros((n_obs, n_actions))\n",
    "\n",
    "        self.epsilon = eps['start']   #initialize the exploration probability to 1\n",
    "        self.epsilon_decay = eps['decay']   #exploration decreasing decay for exponential decreasing\n",
    "        self.epsilon_min = eps['min']   # minimum of exploration proba\n",
    "        \n",
    "        self.gamma = discount   #discounted factor\n",
    "        self.alpha = learning_rate   #learning rate\n",
    "    \n",
    "    def action_selection(self, state):\n",
    "        if np.random.uniform(0,1) < self.epsilon:\n",
    "            action = self.action_space[np.random.randint(0, 3)]   # choose a random action with probability epsilon\n",
    "        else:\n",
    "            action = np.argmax(self.Q_table[state,:])  # choose the best action for that state with prob 1-epsilon\n",
    "        return(action)\n",
    "\n",
    "    def policy_update(self, action, reward, state, next_state):\n",
    "        self.Q_table[state, action] = (1 - self.alpha) * self.Q_table[state, action] + self.alpha*(reward + self.gamma*max(self.Q_table[next_state,:]))\n",
    "\n",
    "    def decrease_exploration(self, e):\n",
    "        self.epsilon = max(self.epsilon_min, np.exp(-self.epsilon_decay*e))\n",
    "        \n",
    "    def test_agent(self, env):\n",
    "        state, _, done = env.reset()\n",
    "        steps = 0\n",
    "        while not done and steps < 100:\n",
    "            action = ag.action_selection(env.state_encode(state))\n",
    "            next_state, reward, done = env.step(action_decode(action))\n",
    "            steps += 1\n",
    "        print(steps)\n",
    "\n",
    "    def train(self, env, n_episodes = 1000, max_steps = 100):\n",
    "        all_rewards = []\n",
    "        all_steps = []\n",
    "        for e in range(n_episodes):   # iterate over episodes\n",
    "            state, _, done = env.reset()\n",
    "            trial_reward = 0\n",
    "            t = 0\n",
    "            while not done and t < max_steps:\n",
    "                action = ag.action_selection(env.state_encode(state))  # step 1: choose an action\n",
    "                old_state = state[:]\n",
    "                next_state, reward, done = env.step(action_decode(action))    # steps 2 and 3: The environment runs the chosen action and returns next state and reward\n",
    "                ag.policy_update(action, reward, env.state_encode(old_state), env.state_encode(next_state))  # step 4: policy update\n",
    "                trial_reward += reward\n",
    "                t += 1\n",
    "            ag.decrease_exploration(e)\n",
    "            all_rewards.append(trial_reward)\n",
    "            all_steps.append(t)\n",
    "        return(all_rewards, all_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c7a922",
   "metadata": {},
   "source": [
    "Task 3\n",
    "\n",
    "A.\tModify the maze, changing size and shape, and different start positions. How many steps does it take to reach the target? Does the performance vary as you expect?\n",
    "\n",
    "B.\tChange the agent learning parameters (e.g. learning rate, discount factor, exploration values). How does performance change in terms of learning speed and ability to reach the target? What happens if exploration is always maximum? And if it decreases very quickly?\n",
    "\n",
    "C. Optional. Change the reward applied to different types of actions and test the learning performance. Are you able to find values for which learning is even faster? Imagine that there was a hole in the maze: how can you make the agent learn to avoid it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2fb681",
   "metadata": {},
   "source": [
    "# Task 3.A: Modifying the Maze"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a46078b",
   "metadata": {},
   "source": [
    "## Modifying the maze size, shape, and start position"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4d1289",
   "metadata": {},
   "source": [
    "### Maze with a size of 10x10:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401a9a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define grid_env and agent classes\n",
    "\n",
    "# Task 3.A: Modifying the Maze for 10x10 maze with start position at [0, 0]\n",
    "maze_height = 10  # Change the height\n",
    "maze_width = 10  # Change the width\n",
    "start_pos = [0, 0]  # Change the start position\n",
    "maze = grid_env(maze_width, maze_height, start_pos)\n",
    "\n",
    "# Create an instance of the agent class\n",
    "ag = agent(n_obs=maze.n_states)\n",
    "\n",
    "# Train the agent with the modified maze\n",
    "episodes = 5000\n",
    "steps = 100\n",
    "rewards, steps_taken = ag.train(maze, episodes, steps)\n",
    "\n",
    "# Display learning performance\n",
    "print(\"Learning Performance:\")\n",
    "display_learning(rewards, \"Reward \")\n",
    "display_learning(steps_taken, \"Steps \")\n",
    "\n",
    "# Test the agent on the modified maze\n",
    "print(\"\\nTesting the agent on the modified maze...\")\n",
    "maze = grid_env(maze_width, maze_height, start_pos, debug=True)\n",
    "ag.test_agent(maze)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb82ed9d",
   "metadata": {},
   "source": [
    "### Maze with a size of 10x15:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e425facb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define grid_env and agent classes\n",
    "\n",
    "# Task 3.A: Modifying the Maze for 10x15 maze with start position at [0, 0]\n",
    "maze_height = 10  # Change the height\n",
    "maze_width = 15  # Change the width\n",
    "start_pos = [0, 0]  # Change the start position\n",
    "maze = grid_env(maze_width, maze_height, start_pos)\n",
    "\n",
    "# Create an instance of the agent class\n",
    "ag = agent(n_obs=maze.n_states)\n",
    "\n",
    "# Train the agent with the modified maze\n",
    "episodes = 5000\n",
    "steps = 100\n",
    "rewards, steps_taken = ag.train(maze, episodes, steps)\n",
    "\n",
    "# Display learning performance\n",
    "print(\"Learning Performance:\")\n",
    "display_learning(rewards, \"Reward \")\n",
    "display_learning(steps_taken, \"Steps \")\n",
    "\n",
    "# Test the agent on the modified maze\n",
    "print(\"\\nTesting the agent on the modified maze...\")\n",
    "maze = grid_env(maze_width, maze_height, start_pos, debug=True)\n",
    "ag.test_agent(maze)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5006700c",
   "metadata": {},
   "source": [
    "### Modifying the Maze for 10x10 maze with start position at [1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482c0b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3.A: Modifying the Maze for 10x10 maze with start position at [1, 2]\n",
    "maze_height = 10  # Change the height\n",
    "maze_width = 10  # Change the width\n",
    "start_pos = [1, 2]  # Change the start position\n",
    "maze = grid_env(maze_width, maze_height, start_pos)\n",
    "\n",
    "# Create an instance of the agent class\n",
    "ag = agent(n_obs=maze.n_states)\n",
    "\n",
    "# Train the agent with the modified maze\n",
    "episodes = 5000\n",
    "steps = 100\n",
    "rewards, steps_taken = ag.train(maze, episodes, steps)\n",
    "\n",
    "# Display learning performance\n",
    "print(\"Learning Performance:\")\n",
    "display_learning(rewards, \"Reward \")\n",
    "display_learning(steps_taken, \"Steps \")\n",
    "\n",
    "# Test the agent on the modified maze\n",
    "print(\"\\nTesting the agent on the modified maze...\")\n",
    "maze = grid_env(maze_width, maze_height, start_pos, debug=True)\n",
    "ag.test_agent(maze)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98483b72",
   "metadata": {},
   "source": [
    "### Modifying the Maze for 10x10 maze with start position at [6, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad0957b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3.A: Modifying the Maze for 10x10 maze with start position at [6, 2]\n",
    "maze_height = 10  # Change the height\n",
    "maze_width = 10  # Change the width\n",
    "start_pos = [6, 2]  # Change the start position\n",
    "maze = grid_env(maze_width, maze_height, start_pos)\n",
    "\n",
    "# Train the agent with the modified maze\n",
    "rewards, steps_taken = ag.train(maze, episodes, steps)\n",
    "\n",
    "# Display learning performance\n",
    "print(\"\\n\\nLearning Performance:\")\n",
    "display_learning(rewards, \"Reward \")\n",
    "display_learning(steps_taken, \"Steps \")\n",
    "\n",
    "# Test the agent on the modified maze\n",
    "print(\"\\nTesting the agent on the modified maze...\")\n",
    "maze = grid_env(maze_width, maze_height, start_pos, debug=True)\n",
    "ag.test_agent(maze)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9df9689",
   "metadata": {},
   "source": [
    "# Task 2.B: Changing Agent Learning Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822c8ad2",
   "metadata": {},
   "source": [
    "## Changing the agent learning parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1641222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Higher Learning Rate\n",
    "\n",
    "# Define modified exploration parameters\n",
    "epsilon = {'start': 1, 'min': 0.001, 'decay': 0.01}\n",
    "# The exploration parameters control the exploration-exploitation trade-off\n",
    "# 'start' is the initial exploration rate, 'min' is the minimum exploration rate, and 'decay' is the rate at which exploration decreases\n",
    "\n",
    "# Define modified discount factor\n",
    "discount_factor = 0.9\n",
    "# The discount factor determines how much importance is given to future rewards\n",
    "\n",
    "# Define modified learning rate (higher value)\n",
    "learning_rate = 0.8\n",
    "# A higher learning rate means the agent will update its Q-values more aggressively based on new experiences\n",
    "# This can lead to faster learning but may also cause instability if the rate is too high\n",
    "\n",
    "# Train the agent with the modified maze\n",
    "episodes = 5000\n",
    "steps = 100\n",
    "rewards, steps_taken = ag.train(maze, episodes, steps)\n",
    "\n",
    "# Display learning performance\n",
    "display_learning(rewards, \"reward \")\n",
    "display_learning(steps_taken, \"steps \")\n",
    "\n",
    "## Training the Agent\n",
    "# Create an instance of the agent class with modified parameters\n",
    "ag = agent(maze.n_states, discount=discount_factor, learning_rate=learning_rate, eps=epsilon)\n",
    "\n",
    "# Train the agent with the modified learning parameters\n",
    "episodes = 5000\n",
    "steps = 100\n",
    "rewards, steps_taken = ag.train(maze, episodes, steps)\n",
    "\n",
    "## Testing the Agent\n",
    "# Test the agent with the modified learning parameters\n",
    "print(\"Testing the agent with modified learning parameters\")\n",
    "maze = grid_env(maze_width, maze_height, start_pos, debug=True)\n",
    "ag.test_agent(maze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595b3b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lower Learning Rate\n",
    "\n",
    "# Define modified exploration parameters\n",
    "epsilon = {'start': 1, 'min': 0.001, 'decay': 0.01}\n",
    "# The exploration parameters control the exploration-exploitation trade-off\n",
    "# 'start' is the initial exploration rate, 'min' is the minimum exploration rate, and 'decay' is the rate at which exploration decreases\n",
    "\n",
    "# Define modified discount factor\n",
    "discount_factor = 0.9\n",
    "# The discount factor determines how much importance is given to future rewards\n",
    "\n",
    "# Define modified learning rate (lower value)\n",
    "learning_rate = 0.2\n",
    "# A lower learning rate means the agent will update its Q-values more conservatively based on new experiences\n",
    "# This can lead to slower learning but may be more stable and less prone to divergence\n",
    "\n",
    "# Train the agent with the modified maze\n",
    "episodes = 5000\n",
    "steps = 100\n",
    "rewards, steps_taken = ag.train(maze, episodes, steps)\n",
    "\n",
    "# Display learning performance\n",
    "display_learning(rewards, \"reward \")\n",
    "display_learning(steps_taken, \"steps \")\n",
    "\n",
    "## Training the Agent\n",
    "# Create an instance of the agent class with modified parameters\n",
    "ag = agent(maze.n_states, discount=discount_factor, learning_rate=learning_rate, eps=epsilon)\n",
    "\n",
    "# Train the agent with the modified learning parameters\n",
    "episodes = 5000\n",
    "steps = 100\n",
    "rewards, steps_taken = ag.train(maze, episodes, steps)\n",
    "\n",
    "## Testing the Agent\n",
    "# Test the agent with the modified learning parameters\n",
    "print(\"Testing the agent with modified learning parameters\")\n",
    "maze = grid_env(maze_width, maze_height, start_pos, debug=True)\n",
    "ag.test_agent(maze)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185c7dee",
   "metadata": {},
   "source": [
    "## Changing Agent Learning Parameters - Exploration Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59346ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Higher Minimum Exploration Rate\n",
    "\n",
    "# Define modified exploration parameters\n",
    "epsilon = {'start': 1, 'min': 0.1, 'decay': 0.01}\n",
    "# The exploration parameters control the exploration-exploitation trade-off\n",
    "# 'start' is the initial exploration rate (set to 1 for maximum exploration at the beginning)\n",
    "# 'min' is the minimum exploration rate (set to 0.1, higher than the default value of 0.001)\n",
    "# A higher minimum exploration rate means the agent will continue to explore more even after many episodes\n",
    "# 'decay' is the rate at which exploration decreases (set to 0.01, a typical value)\n",
    "\n",
    "# Define modified discount factor\n",
    "discount_factor = 0.9\n",
    "# The discount factor determines how much importance is given to future rewards\n",
    "# A value of 0.9 is a common choice, which means future rewards are slightly discounted\n",
    "\n",
    "# Define modified learning rate\n",
    "learning_rate = 0.5\n",
    "# The learning rate controls how much the agent updates its Q-values based on new experiences\n",
    "# A value of 0.5 is a moderate learning rate, which is a good starting point\n",
    "\n",
    "# Train the agent with the modified maze\n",
    "episodes = 5000\n",
    "steps = 100\n",
    "rewards, steps_taken = ag.train(maze, episodes, steps)\n",
    "\n",
    "# Display learning performance\n",
    "display_learning(rewards, \"reward \")\n",
    "display_learning(steps_taken, \"steps \")\n",
    "\n",
    "## Training the Agent\n",
    "# Create an instance of the agent class with modified parameters\n",
    "ag = agent(maze.n_states, discount=discount_factor, learning_rate=learning_rate, eps=epsilon)\n",
    "\n",
    "# Train the agent with the modified learning parameters\n",
    "episodes = 5000\n",
    "steps = 100\n",
    "rewards, steps_taken = ag.train(maze, episodes, steps)\n",
    "\n",
    "## Testing the Agent\n",
    "# Test the agent with the modified learning parameters\n",
    "print(\"Testing the agent with modified learning parameters\")\n",
    "maze = grid_env(maze_width, maze_height, start_pos, debug=True)\n",
    "ag.test_agent(maze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda0ca83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lower Minimum Exploration Rate\n",
    "\n",
    "# Define modified exploration parameters\n",
    "epsilon = {'start': 1, 'min': 0.0001, 'decay': 0.01}\n",
    "# The exploration parameters control the exploration-exploitation trade-off\n",
    "# 'start' is the initial exploration rate (set to 1 for maximum exploration at the beginning)\n",
    "# 'min' is the minimum exploration rate (set to 0.0001, lower than the default value of 0.001)\n",
    "# A lower minimum exploration rate means the agent will exploit more after many episodes\n",
    "# 'decay' is the rate at which exploration decreases (set to 0.01, a typical value)\n",
    "\n",
    "# Define modified discount factor\n",
    "discount_factor = 0.9\n",
    "# The discount factor determines how much importance is given to future rewards\n",
    "# A value of 0.9 is a common choice, which means future rewards are slightly discounted\n",
    "\n",
    "# Define modified learning rate\n",
    "learning_rate = 0.5\n",
    "# The learning rate controls how much the agent updates its Q-values based on new experiences\n",
    "# A value of 0.5 is a moderate learning rate, which is a good starting point\n",
    "\n",
    "# Train the agent with the modified maze\n",
    "episodes = 5000\n",
    "steps = 100\n",
    "rewards, steps_taken = ag.train(maze, episodes, steps)\n",
    "\n",
    "# Display learning performance\n",
    "display_learning(rewards, \"reward \")\n",
    "display_learning(steps_taken, \"steps \")\n",
    "\n",
    "## Training the Agent\n",
    "# Create an instance of the agent class with modified parameters\n",
    "ag = agent(maze.n_states, discount=discount_factor, learning_rate=learning_rate, eps=epsilon)\n",
    "\n",
    "# Train the agent with the modified learning parameters\n",
    "episodes = 5000\n",
    "steps = 100\n",
    "rewards, steps_taken = ag.train(maze, episodes, steps)\n",
    "\n",
    "## Testing the Agent\n",
    "# Test the agent with the modified learning parameters\n",
    "print(\"Testing the agent with modified learning parameters\")\n",
    "maze = grid_env(maze_width, maze_height, start_pos, debug=True)\n",
    "ag.test_agent(maze)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4e377f",
   "metadata": {},
   "source": [
    "## Changing Agent Learning Parameters - Discount Factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775aeefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Higher Discount Factor\n",
    "\n",
    "# Define modified exploration parameters\n",
    "epsilon = {'start': 1, 'min': 0.001, 'decay': 0.01}\n",
    "# The exploration parameters control the exploration-exploitation trade-off\n",
    "# 'start' is the initial exploration rate (set to 1 for maximum exploration at the beginning)\n",
    "# 'min' is the minimum exploration rate (set to 0.001, a common value)\n",
    "# 'decay' is the rate at which exploration decreases (set to 0.01, a typical value)\n",
    "\n",
    "# Define modified discount factor (higher value)\n",
    "discount_factor = 0.99\n",
    "# The discount factor determines how much importance is given to future rewards\n",
    "# A higher discount factor (closer to 1) means the agent will value future rewards more\n",
    "# This can lead to more farsighted decision-making, but may also make the learning process slower and more sensitive to long-term rewards\n",
    "\n",
    "# Define modified learning rate\n",
    "learning_rate = 0.5\n",
    "# The learning rate controls how much the agent updates its Q-values based on new experiences\n",
    "# A value of 0.5 is a moderate learning rate, which is a good starting point\n",
    "\n",
    "# Train the agent with the modified maze\n",
    "episodes = 5000\n",
    "steps = 100\n",
    "rewards, steps_taken = ag.train(maze, episodes, steps)\n",
    "\n",
    "# Display learning performance\n",
    "display_learning(rewards, \"reward \")\n",
    "display_learning(steps_taken, \"steps \")\n",
    "\n",
    "## Training the Agent\n",
    "# Create an instance of the agent class with modified parameters\n",
    "ag = agent(maze.n_states, discount=discount_factor, learning_rate=learning_rate, eps=epsilon)\n",
    "\n",
    "# Train the agent with the modified learning parameters\n",
    "episodes = 5000\n",
    "steps = 100\n",
    "rewards, steps_taken = ag.train(maze, episodes, steps)\n",
    "\n",
    "## Testing the Agent\n",
    "# Test the agent with the modified learning parameters\n",
    "print(\"Testing the agent with modified learning parameters\")\n",
    "maze = grid_env(maze_width, maze_height, start_pos, debug=True)\n",
    "ag.test_agent(maze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e866cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lower Discount Factor\n",
    "\n",
    "# Define modified exploration parameters\n",
    "epsilon = {'start': 1, 'min': 0.001, 'decay': 0.01}\n",
    "# The exploration parameters control the exploration-exploitation trade-off\n",
    "# 'start' is the initial exploration rate (set to 1 for maximum exploration at the beginning)\n",
    "# 'min' is the minimum exploration rate (set to 0.001, a common value)\n",
    "# 'decay' is the rate at which exploration decreases (set to 0.01, a typical value)\n",
    "\n",
    "# Define modified discount factor (lower value)\n",
    "discount_factor = 0.2\n",
    "# The discount factor determines how much importance is given to future rewards\n",
    "# A lower discount factor (closer to 0) means the agent will value immediate rewards more\n",
    "# This can lead to more short-sighted decision-making, but may also make the learning process faster and less sensitive to long-term rewards\n",
    "\n",
    "# Define modified learning rate\n",
    "learning_rate = 0.5\n",
    "# The learning rate controls how much the agent updates its Q-values based on new experiences\n",
    "# A value of 0.5 is a moderate learning rate, which is a good starting point\n",
    "\n",
    "# Train the agent with the modified maze\n",
    "episodes = 5000\n",
    "steps = 100\n",
    "rewards, steps_taken = ag.train(maze, episodes, steps)\n",
    "\n",
    "# Display learning performance\n",
    "display_learning(rewards, \"reward \")\n",
    "display_learning(steps_taken, \"steps \")\n",
    "\n",
    "## Training the Agent\n",
    "# Create an instance of the agent class with modified parameters\n",
    "ag = agent(maze.n_states, discount=discount_factor, learning_rate=learning_rate, eps=epsilon)\n",
    "\n",
    "# Train the agent with the modified learning parameters\n",
    "episodes = 5000\n",
    "steps = 100\n",
    "rewards, steps_taken = ag.train(maze, episodes, steps)\n",
    "\n",
    "## Testing the Agent\n",
    "# Test the agent with the modified learning parameters\n",
    "print(\"Testing the agent with modified learning parameters\")\n",
    "maze = grid_env(maze_width, maze_height, start_pos, debug=True)\n",
    "ag.test_agent(maze)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520b8fa2",
   "metadata": {},
   "source": [
    "# Task 3.C: Changing Reward and Testing Learning Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5f0cc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def action_decode(act_code):\n",
    "    dirs = {0: \"N\", 1: \"E\", 2: \"S\", 3: \"W\"}\n",
    "    return dirs[act_code]\n",
    "\n",
    "def action_encode(act):\n",
    "    dir_codes = {\"N\": 0, \"E\": 1, \"S\": 2, \"W\": 3}\n",
    "    return dir_codes[act]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d0a936",
   "metadata": {},
   "source": [
    "## Grid Environment Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "15aaeec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class grid_env():\n",
    "    def __init__(self, width=5, height=5, start=[0, 0], debug=False):\n",
    "        # Initialize the environment with given parameters\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.start = start\n",
    "        self.goal = [self.width - 1, self.height - 1]\n",
    "        self.debug = debug\n",
    "        self.n_states = self.width * self.height\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset the environment to the starting position\n",
    "        self.pos = self.start[:]\n",
    "        return self.pos, 0, False\n",
    "\n",
    "    def state_decode(self, obs_code):\n",
    "        # Decode the state code into coordinates\n",
    "        r = obs_code // self.width\n",
    "        c = obs_code % self.width\n",
    "        return [c, r]\n",
    "\n",
    "    def state_encode(self, position):\n",
    "        # Encode the coordinates into a state code\n",
    "        code = position[0] + position[1] * (self.width)\n",
    "        return code\n",
    "\n",
    "    def step(self, action):\n",
    "        # Take a step in the environment based on the action\n",
    "        new_pos = self.pos[:]\n",
    "        if action == \"S\" and (self.pos[1] < self.height - 1):\n",
    "            new_pos[1] += 1\n",
    "        elif action == \"N\" and self.pos[1] > 0:\n",
    "            new_pos[1] -= 1\n",
    "        elif action == \"W\" and self.pos[0] > 0:\n",
    "            new_pos[0] -= 1\n",
    "        elif action == \"E\" and (self.pos[0] < self.width - 1):\n",
    "            new_pos[0] += 1\n",
    "\n",
    "        # Check if the new position is the \"hole\"\n",
    "        if new_pos == [2, 3]:  # Example hole position\n",
    "            reward = -1000  # Large negative reward for falling in the hole\n",
    "            done = True\n",
    "        else:\n",
    "            self.pos = new_pos\n",
    "            done = (self.pos == self.goal)\n",
    "            if done:\n",
    "                reward = self.width + self.height  # Reward at goal\n",
    "            else:\n",
    "                reward = -1  # Negative reward for each step\n",
    "\n",
    "        if self.debug:\n",
    "            print(self.render())\n",
    "\n",
    "        return self.pos, reward, done\n",
    "\n",
    "    def render(self):\n",
    "        # Render the environment\n",
    "        res = \"\"\n",
    "        for y in range(self.height):\n",
    "            for x in range(self.width):\n",
    "                if self.goal[0] == x and self.goal[1] == y:\n",
    "                    if self.pos[0] == x and self.pos[1] == y:\n",
    "                        res += \"@\"\n",
    "                    else:\n",
    "                        res += \"o\"\n",
    "                    continue\n",
    "                if self.pos[0] == x and self.pos[1] == y:\n",
    "                    res += \"x\"\n",
    "                else:\n",
    "                    res += \"_\"\n",
    "            res += \"\\n\"\n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111aa09e",
   "metadata": {},
   "source": [
    "## Agent Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db3991a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class agent():\n",
    "    def __init__(self, n_obs, discount=1, learning_rate=0.1, eps={'start': 1, 'min': 0.01, 'decay': 0.001}):\n",
    "        # Initialize the agent with given parameters\n",
    "        self.action_space = np.asarray([0, 1, 2, 3])\n",
    "        n_actions = np.shape(self.action_space)[0]\n",
    "        self.Q_table = np.zeros((n_obs, n_actions))\n",
    "        self.epsilon = eps['start']\n",
    "        self.epsilon_decay = eps['decay']\n",
    "        self.epsilon_min = eps['min']\n",
    "        self.gamma = discount\n",
    "        self.alpha = learning_rate\n",
    "\n",
    "    def action_selection(self, state):\n",
    "        # Select an action based on epsilon-greedy policy\n",
    "        if np.random.uniform(0, 1) < self.epsilon:\n",
    "            action = self.action_space[np.random.randint(0, 3)]\n",
    "        else:\n",
    "            action = np.argmax(self.Q_table[state, :])\n",
    "        return action\n",
    "\n",
    "    def policy_update(self, action, reward, state, next_state):\n",
    "        # Update the Q-table based on the observed transition\n",
    "        self.Q_table[state, action] = (1 - self.alpha) * self.Q_table[state, action] + self.alpha * (\n",
    "                    reward + self.gamma * max(self.Q_table[next_state, :]))\n",
    "\n",
    "    def decrease_exploration(self, e):\n",
    "        # Decrease exploration rate\n",
    "        self.epsilon = max(self.epsilon_min, np.exp(-self.epsilon_decay * e))\n",
    "\n",
    "    def test_agent(self, env):\n",
    "        # Test the agent's performance in the environment\n",
    "        state, _, done = env.reset()\n",
    "        steps = 0\n",
    "        while not done and steps < 1000:\n",
    "            action = ag.action_selection(env.state_encode(state))\n",
    "            next_state, reward, done = env.step(action_decode(action))\n",
    "            steps += 1\n",
    "        print(f\"Steps taken: {steps}\")\n",
    "\n",
    "    def train(self, env, n_episodes=10000, max_steps=1000):\n",
    "        # Train the agent in the environment\n",
    "        all_rewards = []\n",
    "        all_steps = []\n",
    "        for e in range(n_episodes):\n",
    "            state, _, done = env.reset()\n",
    "            trial_reward = 0\n",
    "            t = 0\n",
    "            while not done and t < max_steps:\n",
    "                action = ag.action_selection(env.state_encode(state))\n",
    "                old_state = state[:]\n",
    "                next_state, reward, done = env.step(action_decode(action))\n",
    "                ag.policy_update(action, reward, env.state_encode(old_state), env.state_encode(next_state))\n",
    "                trial_reward += reward\n",
    "                t += 1\n",
    "            ag.decrease_exploration(e)\n",
    "            all_rewards.append(trial_reward)\n",
    "            all_steps.append(t)\n",
    "        return all_rewards, all_steps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85144d8",
   "metadata": {},
   "source": [
    "## Changing Reward and Simulating a Hole in the Maze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f43af064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create grid environment and agent\n",
    "maze_width = 5\n",
    "maze_height = 5\n",
    "start_pos = [0, 0]\n",
    "maze = grid_env(maze_width, maze_height, start_pos, debug=True)\n",
    "ag = agent(n_obs=maze.n_states)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04151466",
   "metadata": {},
   "source": [
    "## Training the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04151466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the agent\n",
    "episodes = 5000\n",
    "steps = 100\n",
    "rewards, steps_taken = ag.train(maze, episodes, steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0588287",
   "metadata": {},
   "source": [
    "## Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c32664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the agent with modified reward and hole\n",
    "print(\"Testing the agent with modified reward and hole...\")\n",
    "maze = grid_env(maze_width, maze_height, start_pos, debug=True)\n",
    "ag.test_agent(maze)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
